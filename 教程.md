[toc]

# SDK的包更新注意事项

1. 需要打包SDK framewrok后进行`strip -x`来减少包体积

2. 同时需要注意修改demo，不要暴露测试会议给外面，同时注意修改引用关系，以及search path

3. 注意更新文档，以及cocoapods和github上的wiki文档

4. 最后需要核对lib库数量以及framework库的数量以及大小是否有太大变动，并找出原因修复。例如本来旧版本只有100MB，这次你打包就150MB，你就需要找找原因了，懂我说的吧，是否copy了不需要的库呢？RtSDK中打包了3个库不需要拷贝，另外libfaad同级的几个库也不需要拷贝，请参考历史版本。



---



# 如何发布以及更改Cocoapods库

[如何创建一个Cocoapods库](https://editor.csdn.net/md/?articleId=104904226)

pod是依赖于github上仓库的tag的，每次提交github之后，需要进行tag

命令行操作如下：
```c

git add .
git commit -m "修改内容"
git push

//做一个和pod的spec文件版本一致的版本号的tag
git tag '1.0.0'
git push --tags

//然后更新spec文件后，进行pod trunk push 推送到pod仓库
pod trunk push xxx.spec --allow-warnings
//--allow-warnings表示忽略警告
//完成后告诉你这个仓库validate，就可用了
```

如果你想撤销pod的提交

```c
1.删除pod trunk下的仓库,例如xxxx '1.0.0'
pod trunk delete xxxx '1.0.0'

2.删除git下的release tag文件 - tag是可以在github下载的历史版本
git push origin -d tag '1.0.1'
//删除本地
git tag -d '1.0.0'
  
```

---



# 关于如何编写wiki文档

[如何编写wiki文档](https://blog.csdn.net/shengpeng3344/article/details/105496054)

可以参考已经编写好的3个wiki进行修改

https://github.com/net263/FASTSDK/wiki

https://github.com/net263/RtSDK/wiki

https://github.com/net263/PlayerSDK/wiki

> 注意更新side bar之后记得要将home页也随时更新



---




# 关于SDK视频解码

SDK解码看`GSH264Decoder`，我们传入的是底层传给我们的`H.264`数据

关于H.264数据可以看 [H264数据组成](https://blog.csdn.net/shengpeng3344/article/details/104957016)

例如一段H.264数据的开头
```c
0x00 00 00 01 67 ...(SPS DATA)... 0x00 00 00 01 68 ...(PPS DATA)... 0x00 00 00 01 65 ...(IDR DATA)... 0x00 00 00 01 06 ...(SEI DATA)...
0x00 00 00 01 61 ...(P DATA)... 0x00 00 00 01 61 ...(P DATA)...
```

根据数据由`0x00 00 00 01`开头，故需要取`data[4]`才为`NAL TYPE`类型,`0x68` 表示 `0110 1000` 即表示`forbit=0,reference=11,nalu_type=01000`
```c
int nalu_type = (frame[startCodeIndex + 4] & 0x1F);
```

当收到一帧SPS时，意为这是一个`I帧`，前面会有`SPS PPS IDR`，故我们有这样的判断:

```c
for (int i = startCodeIndex + 4; i < startCodeIndex + 40; i++) {
            if (frame[i] == 0x00 && frame[i+1] == 0x00 && frame[i+2] == 0x00 && frame[i+3] == 0x01) {
                secondStartCodeIndex = i;
                _spsSize = secondStartCodeIndex;   // includes the header in the size
                break;
            }
        }
```
找到第二个 `0x00 00 00 01` ，按照这样的原理找到了 `SPS PPS`之后，进行`VideoFormatDescription`的创建

之后我们需要解码`I帧`和`P帧`，那么就需要创建解码实例：

```c

	VTDecompressionOutputCallbackRecord callBackRecord;
    callBackRecord.decompressionOutputCallback = GSDecompressionSessionDecodeFrameCallback;
    callBackRecord.decompressionOutputRefCon = (__bridge void *)self;
    
    // you can set some desired attributes for the destination pixel buffer.  I didn't use this but you may
    // if you need to set some attributes, be sure to uncomment the dictionary in VTDecompressionSessionCreate
    CFMutableDictionaryRef destinationPixelBufferAttributes;
    destinationPixelBufferAttributes = CFDictionaryCreateMutable(
                                                                 NULL,
                                                                 0,
                                                                 &kCFTypeDictionaryKeyCallBacks,
                                                                 &kCFTypeDictionaryValueCallBacks);
    //kCVPixelBufferWidthKey/kCVPixelBufferHeightKey: 视频源的分辨率 width*height 类型需要是CFNumber
	//CFDictionarySetSInt32(destinationPixelBufferAttributes, kCVPixelBufferHeightKey, 类型需要是CFNumber);

    /*kCVPixelBufferPixelFormatTypeKey，已测可用值为
     kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange，即420v
     kCVPixelFormatType_420YpCbCr8BiPlanarFullRange，即420f
     kCVPixelFormatType_32BGRA，iOS在内部进行YUV至BGRA格式转换
     YUV420一般用于标清视频，YUV422用于高清视频，这里的限制让人感到意外。但是，在相同条件下，YUV420计算耗时和传输压力比YUV422都小。*/
    CFDictionarySetSInt32(destinationPixelBufferAttributes,
                          kCVPixelBufferPixelFormatTypeKey, kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange);
    //kCVPixelBufferOpenGLCompatibilityKey : 它允许在 OpenGL 的上下文中直接绘制解码后的图像，而不是从总线和 CPU 之间复制数据。这有时候被称为零拷贝通道，因为在绘制过程中没有解码的图像被拷贝.
    CFDictionarySetBoolean(destinationPixelBufferAttributes,
                           kCVPixelBufferOpenGLESCompatibilityKey, YES);
    
    
    OSStatus status =  VTDecompressionSessionCreate(kCFAllocatorDefault,
                                                    self->_formatDesc,
                                                    NULL,
                                                    destinationPixelBufferAttributes,
                                                    &callBackRecord,
                                                    &self->_decompressionSession);
    /*
```

`GSDecompressionSessionDecodeFrameCallback` 为回调函数


```c
void GSDecompressionSessionDecodeFrameCallback(void *decompressionOutputRefCon,
                                               void *sourceFrameRefCon,
                                               OSStatus status,
                                               VTDecodeInfoFlags infoFlags,
                                               CVImageBufferRef imageBuffer,
                                               CMTime presentationTimeStamp,
                                               CMTime presentationDuration) {
    if (status != noErr || !imageBuffer) {
        // error -8969 codecBadDataErr
        // -12909 The operation couldn’t be completed. (OSStatus error -12909.)
        return;
    }
    // do something with your resulting CVImageBufferRef that is your decompressed frame
    //    NSLog(@"Success decompresssing frame at time: %.3f error: %d infoFlags: %u", (float)presentationTimeStamp.value/presentationTimeStamp.timescale, (int)status, (unsigned int)infoFlags);
    
    size_t width = CVPixelBufferGetWidth(imageBuffer);
    size_t height = CVPixelBufferGetHeight(imageBuffer);
    gsVideoWidth = width;
    gsVideoHeight = height;
    GSH264Decoder *decoder = (__bridge GSH264Decoder*)decompressionOutputRefCon;
    if (decoder.delegate && [decoder.delegate respondsToSelector:@selector(h264Decoder:onCVImageBufferRef:width:height:)]) {
        //解码后的数据sourceFrameRefCon -> CVPixelBufferRef
        CVPixelBufferRetain(imageBuffer);
//        NSLog(@"[GSH264Decoder] 1 %d",CFGetRetainCount(imageBuffer));
        dispatch_async(decoder.callbackQueue, ^{
//            NSLog(@"[GSH264Decoder] 2 %d",CFGetRetainCount(imageBuffer));
            [decoder.delegate h264Decoder:decoder onCVImageBufferRef:imageBuffer width:width height:height];
            if (imageBuffer) {
//                NSLog(@"[GSH264Decoder] 6 %d",CFGetRetainCount(imageBuffer));
                CVPixelBufferRelease(imageBuffer);
            }

        });
    }
}
```

这里关于decode如何将数据放入解码队列呢？见如下:



```c
CMItemCount count = CMSampleBufferGetNumSamples(sampleBuffer);
if (count > 0) {
    //解码
    //向视频解码器提示使用低功耗模式是可以的
    uint32_t decoder_flags          = 0;
    decoder_flags |= kVTDecodeFrame_EnableAsynchronousDecompression;
    decoder_flags |= kVTDecodeFrame_1xRealTimePlayback;
    //异步解码
    VTDecodeInfoFlags  flagOut = kVTDecodeInfo_Asynchronous;
    NSDate* currentTime = [NSDate date];
    status = VTDecompressionSessionDecodeFrame(_decompressionSession, sampleBuffer, decoder_flags, (void*)CFBridgingRetain(currentTime), &flagOut);
    if (status == kVTInvalidSessionErr) {
        GSLog(@"Video hard decode  InvalidSessionErr status = %d ，nalutype : %d", (int)status,nalu_type);
    } else if (status == kVTVideoDecoderBadDataErr) {
        GSLog(@"Video hard decode  BadData status = %d ，nalutype : %d", (int)status,nalu_type);
    } else if (status != noErr) {
        GSLog(@"Video hard decode failed status = %d , nalutype : %d", (int)status,nalu_type);
    }
    CFBridgingRelease((__bridge CFTypeRef _Nullable)(currentTime));
}else {
    GSLog(@"sampleBuffer items == 0");
}
```

TODO : 

- 这里我们可以在放入解码队列前判断其帧类型，如果是`I P B`则进行解码，防止异常错误，你还能想到其他需要解码的帧吗。对于H.264的结构你可以看看这里 [H264数据组成](https://blog.csdn.net/shengpeng3344/article/details/104957016)
- 对于重复帧的处理，我想的是对数据进行hash计算，然后判断前后帧的hash是否相等来避免，这也是一个可以优化的过程，但是否有性能问题，值得参考了。



解码之后就是`OpenGL`的`YUV`数据渲染了。硬解一般都是`H.264包装YUV`，而软解由底层`ffmpeg解码`，所以传给我们的就是`RGBA`了，我们可以直接用来`OpenGL`渲染。



---



# OpenGL渲染



对于OpenGL渲染，你需要了解基础概念，一个YUV数据渲染大概通过以下几个流程。



顶点计算 -> 光栅化生成像素 -> 加载纹理